{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problem set 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Clyde Fare](mailto:c.fare12@imperial.ac.uk) and [Jo\u00e3o Pedro Malhado](mailto:malhado@imperial.ac.uk), Imperial College London\n",
      "\n",
      "Notebook is adapted from material gathered by Andrew McKinley and Oliver Robotham, as well as Paul Wilde, and is licensed under a [Creative Commons Attribution 3.0 (CC-by) license](http://creativecommons.org/licenses/by/3.0/us)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load the pylab and non-linear fitting machinery and get that out of the way."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we'll be dealing with larger datasets, such as those you could have obtained from a measuring instrument, and looking at some properties of the statistical description of sets of measurements.\n",
      "\n",
      "The file [measurements.csv](files/measurements.csv) containes 5 sets of measurements of different systems and/or in different conditions. For each set the meaurement was repeated 100000 times. (Good thing we don't have to analyse this data by hand!)\n",
      "\n",
      "* Have a peek at the file and load its data into a variable. Output the value of the variable to see how the imported data looks like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data=loadtxt('measurements.csv',delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A quick and informative way of acquiring an overall view of the data distribution is to plot histograms.\n",
      "\n",
      "* On the same figure, plot the histograms of all datasets conveniently labelling each of them. A better comparison is obtained setting an equal histogram range for each data set, and setting the transparency alpha value to see through superimposed data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figsize(20,15)\n",
      "\n",
      "hist(data[:,0],200,range=(-4,10),alpha=0.5,label='data1')\n",
      "hist(data[:,1],200,range=(-4,10),alpha=0.5,label='data2')\n",
      "hist(data[:,2],200,range=(-4,10),alpha=0.5,label='data3')\n",
      "hist(data[:,3],200,range=(-4,10),alpha=0.5,label='data4')\n",
      "hist(data[:,4],200,range=(-4,10),alpha=0.5,label='data5')\n",
      "\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Any measurement is affected by error and two consecutive measurements of the same system are unlikelly to give the same result. The *mean* $\\bar{x}$ of a set of independent measures the property $x$ is the best estimate of the real value of that property of the system. As we will see bellow, the quality of this estimate is improved by increasing the number of measurements. The sample's *standard deviation* $s_x$ quantifies the dispersion of the experimental measurements with respect to the mean. (Note that the standard deviation does not correspond to the error of the measurement.)\n",
      "\n",
      "* For each data set (column) of 100000 measurements, calculate the mean and the standard deviation. Overlap the histograms plotted above with a vertical line with $x$ at the mean value of each measurement set. How do the $\\bar{x}$ and $s_x$ values relate the shape of the distributions in the histograms?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(data[:,:],axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "std(data[:,:],axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figsize(20,15)\n",
      "\n",
      "hist(data[:,0],200,range=(-4,10),alpha=0.5,label='data1')\n",
      "plot(array([mean(data[:,0]),mean(data[:,0])]),array([0,30000]),linestyle='--',color='black')\n",
      "hist(data[:,1],200,range=(-4,10),alpha=0.5,label='data2')\n",
      "plot(array([mean(data[:,1]),mean(data[:,1])]),array([0,30000]),linestyle='--',color='black')\n",
      "hist(data[:,2],200,range=(-4,10),alpha=0.5,label='data3')\n",
      "plot(array([mean(data[:,2]),mean(data[:,2])]),array([0,30000]),linestyle='--',color='black')\n",
      "hist(data[:,3],200,range=(-4,10),alpha=0.5,label='data4')\n",
      "plot(array([mean(data[:,3]),mean(data[:,3])]),array([0,30000]),linestyle='--',color='black')\n",
      "hist(data[:,4],200,range=(-4,10),alpha=0.5,label='data5')\n",
      "plot(array([mean(data[:,4]),mean(data[:,4])]),array([0,30000]),linestyle='--',color='black')\n",
      "\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now study how the statistical properties vary with the sample size.\n",
      "\n",
      "* For the first data set (column), overlap the histograms of the full sample (N=100000), selecting only the first 10000 and 1000 measurements. (For this comparison you likelly want to plot the histogram normalized by sample size. Choose the appropriate option of function *hist* to achieve this.) By first observing the histograms, and then by calculating their value, what can you infer about the variation of the mean value and standard deviation with sample size (for sufficiently big samples)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist(data[:,0],100,normed=True,range=(0,5),alpha=0.5,label='N=100000')\n",
      "hist(data[:10000,0],100,normed=True,range=(0,5),alpha=0.5,label='N=10000')\n",
      "hist(data[:1000,0],100,normed=True,range=(0,5),alpha=0.5,label='N=1000')\n",
      "\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "array([mean(data[:1000,0]),mean(data[:25000,0]),mean(data[:50000,0]),mean(data[:75000,0]),mean(data[:,0])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "array([std(data[:1000,0]),std(data[:25000,0]),std(data[:50000,0]),std(data[:75000,0]),std(data[:,0])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statiscal uncertainty associated with the mean value is given by the *standard error of the mean* $\\sigma_{\\bar{x}}$ which is calculated by the expression:\n",
      "\n",
      "$$\\sigma_{\\bar{x}}=\\frac{s_x}{\\sqrt{N}}=\\sqrt{\\frac{1}{N (N-1)}\\sum_i^N (x_i-\\bar{x})^2} .$$\n",
      "\n",
      "This is the value that should be given as the statistical error of your measurements!\n",
      "\n",
      "Let us investigate how this quantity vary with sample size. We will be considering data sets 1 and 4, which have similar mean value as per histograms above. We will consider that both sets measure the same quantity in the same system but in different conditions (eg. a different instrument is used).\n",
      "\n",
      "* For both data sets 1 and 4, plot the standard deviation and the standard error as a function of the sample size. (Choose for example sample sizes N=array([1000,25000,50000,75000,100000]})). You may want to use a logarithmic scale in this plot. There is no standard error function available in pylab (although you can find it in other modules) so you will have to calculate it. What does this plot tell you about the usefulness of repeating the same measurement?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=array([1000,25000,50000,75000,100000])\n",
      "plot(N,array([std(data[:1000,0]),std(data[:25000,0]),std(data[:50000,0]),std(data[:75000,0]),std(data[:,0])]),color='blue',linestyle='--',label=r'$s_x(data1)$')\n",
      "plot(N,array([std(data[:1000,0])/sqrt(1000),std(data[:25000,0])/sqrt(25000),std(data[:50000,0])/sqrt(50000),std(data[:75000,0])/sqrt(75000),std(data[:,0])/sqrt(100000)]),color='blue',label=r'$\\sigma_{\\bar{x}}(data1)$')\n",
      "plot(N,array([std(data[:1000,3]),std(data[:25000,3]),std(data[:50000,3]),std(data[:75000,3]),std(data[:,3])]),color='cyan',linestyle='--',label=r'$s_x(data4)$')\n",
      "plot(N,array([std(data[:1000,3])/sqrt(1000),std(data[:25000,3])/sqrt(25000),std(data[:50000,3])/sqrt(50000),std(data[:75000,3])/sqrt(75000),std(data[:,3])/sqrt(100000)]),color='cyan',label=r'$\\sigma_{\\bar{x}}(data4)$')\n",
      "yscale('log')\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* From the histograms above we note that the dispersion of data set 4 is greater than that of data set 1, and consequently the standard error of the mean is greater in the former case for equal sample size. By drawing an horizontal line in the plot above, do a rough estimate of what sample size would be sufficient to achieve the same precision in the conditions of data set 1 as the one obtained for data set 4 with 100000 measurements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=array([1000,25000,50000,75000,100000])\n",
      "plot(N,array([std(data[:1000,0]),std(data[:25000,0]),std(data[:50000,0]),std(data[:75000,0]),std(data[:,0])]),color='blue',linestyle='--',label=r'$s_x(data1)$')\n",
      "plot(N,array([std(data[:1000,0])/sqrt(1000),std(data[:25000,0])/sqrt(25000),std(data[:50000,0])/sqrt(50000),std(data[:75000,0])/sqrt(75000),std(data[:,0])/sqrt(100000)]),color='blue',label=r'$\\sigma_{\\bar{x}}(data1)$')\n",
      "plot(N,array([std(data[:1000,3]),std(data[:25000,3]),std(data[:50000,3]),std(data[:75000,3]),std(data[:,3])]),color='cyan',linestyle='--',label=r'$s_x(data4)$')\n",
      "plot(N,array([std(data[:1000,3])/sqrt(1000),std(data[:25000,3])/sqrt(25000),std(data[:50000,3])/sqrt(50000),std(data[:75000,3])/sqrt(75000),std(data[:,3])/sqrt(100000)]),color='cyan',label=r'$\\sigma_{\\bar{x}}(data4)$')\n",
      "\n",
      "plot(array([0,100000]),array([std(data[:,3])/sqrt(100000),std(data[:,3])/sqrt(100000)]),color='red',linestyle=':')\n",
      "\n",
      "yscale('log')\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a final note, from the definition of standard error and the study we just carried out, what is the limit of the standard error value as the sample size goes to infinity? Although the statistical error vanishes for infinite sample sizes, this does not mean you can achieve in a limit an absolute confidence in your measurements (imagine you do infinite many length measurements of an object a few milimiters long with a bent ruler!). The standard error does not take into account sistematic errors due to your instrument or faulty operation. Sistematic errors need to be estimated based on the knowledge of the measurement process and intruments used."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we will be studying the kinetics of the decomposition reaction of $N_2 O_5$\n",
      "\n",
      "$$2 N_2 O_{5 (g)} \\longrightarrow 4NO_{2 (g)} + O_{2 (g)}$$\n",
      "\n",
      "The rate of a chemical reaction is how fast the concentration of one of the reactants or products changes during the chemical reaction. In the case of the reaction under study the rate of the reaction is proportional to the concentration of $N_2 O_5$, it is said to obey a first order rate law\n",
      "\n",
      "$$rate=- \\frac{d[N_2 O_5]}{d t} =k [N_2 O_5] ,$$\n",
      "\n",
      "where $k$ is the *rate constant*. This is a first order ordinary differential equation in \ufffc$[N_2 O_5]$, and its solution is\n",
      "\n",
      "$$[N_2 O_5]=[N_2 O_5]_0 e^{-k t}\ufffc$$\n",
      "\n",
      "for an initial concentration $[N_2 O_5]_0$. (You can see a brief [schetch of the solution](https://wiki.ch.ic.ac.uk/wiki/index.php?title=Solving_the_Rate_Equation) of the rate equation.)\n",
      "\n",
      "The first objective of the exercise is to determine the value of the rate constant $k$. In order to do this, the concentration of $N_2 O_5$ was recorded as a function of time at $65^{\\circ}C$, and registered in file [N2O5vst_65C.dat](files/N2O5vst_65C.dat), where the first column is time in seconds and the second column is concentration in $\\text{mol.dm}^{-3}$.\n",
      "\n",
      "In order to have an estimate of the uncertainty of each reading, a series of measurements were made at the same time of reaction (490s), and are registered in the table bellow. We will consider that the error bars extracted from these readings are representative of the error for readings at all times of reaction.\n",
      "\n",
      "<table>\n",
      "<tr><th>Reading No.</th><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr>\n",
      "<tr><th>$[N_2 O_5]/(\\text{mol.dm}^{-3})$</th><td>0.00379</td><td>0.00345</td><td>0.00398</td><td>0.00349</td><td>0.00411</td><td>0.00372</td><td>0.00401</td><td>0.00388</td><td>0.00362</td><td>0.00395</td></tr>\n",
      "</table>\n",
      "\n",
      "* Load the values of concentration as a function of time into a variable. From the repeated measuremenst at the same time calculate the size of the error bars to apply to every point. Plot the concentrarion of $N_2 O_5$ as a function ot time with error bars and proper axes labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n2o5=loadtxt('N2O5vst_65C.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n2o5_err=std(array([0.00379,0.00345,0.00398,0.00349,0.00411,0.00372,0.00401,0.00388,0.00362,0.00395])) #need to check which error is appropriate!\n",
      "n2o5_err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errorbar(n2o5[:,0],n2o5[:,1],yerr=ones(19)*n2o5_err,marker='o',linestyle='none')\n",
      "xlabel('t/s')\n",
      "ylabel('$[N_2 O_5]/mol.dm^{-3}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Create an appropriate exponential function that will allow obtaining the value of the rate constant by fitting the experimental data. Fit the data and report the value of the rate constant and the uncertainty associated with it. Superimpose the fit with the experimental data to check your fit is meaningful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rate1(t,k,C0):\n",
      "    \"Exponential function consistent with first order rate law.\"\n",
      "    return C0*e**(-k*t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n2o5_fit=scipy.optimize.curve_fit(rate1,n2o5[:,0],n2o5[:,1],p0=[0.000001,0.1])\n",
      "n2o5_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqrt(n2o5_fit[1][0,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t=n2o5[:,0]\n",
      "k=n2o5_fit[0][0]\n",
      "C0=n2o5_fit[0][1]\n",
      "\n",
      "errorbar(t,n2o5[:,1],yerr=ones(19)*n2o5_err,marker='o',linestyle='none')\n",
      "plot(t,rate1(t,k,C0),linestyle='--')\n",
      "xlabel('t/s')\n",
      "ylabel('$[N_2 O_5]/mol.dm^{-3}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some other colleagues followed the same proceedure as above but at different temperatures. The following table shows values of the rate constant at its uncertainty with respect for different temperatures.\n",
      "\n",
      "<table>\n",
      "<tr><th>$T/^{\\circ} C$</th><th>$k/s^{-1}$</th><th>$\\sigma_k/s^{-1}$</th><tr>\n",
      "<tr><td>20</td><td>0.0000188</td><td>0.0000089</td></tr>\n",
      "<tr><td>30</td><td>0.0000694</td><td>0.0000073</td></tr>\n",
      "<tr><td>40</td><td>0.000260</td><td>0.000023</td></tr>\n",
      "<tr><td>50</td><td>0.000888</td><td>0.000067</td></tr>\n",
      "<tr><td>60</td><td>0.00293</td><td>0.00021</td></tr>\n",
      "</table>\n",
      "\n",
      "The variation of the rate constant with temperature can give important information about the chemical reaction. The rate constant of an elementary reaction varies in general with temperature according to Arrhenius equation\n",
      "\n",
      "$$k(T)=A e^{-\\frac{E_a}{R T}} ,$$\n",
      "\n",
      "where $E_a$ is the chemical reaction activation energy, $A$ is the so called pre-exponential factor related to the frequency of molecular collisions, $R$ is the ideal gas constant.\n",
      "\n",
      "* From the rate constant values on the table above, and the rate constant value which you have determined, fit the data to an Arrhenius curve to the determine the pre-exponential factor and the activation energy for this reaction. (Note that the uncertaities associated with each rate constant are different.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ktemp=array([array([20,30,40,50,60,65])+273.15,array([0.0000188,0.0000694,0.000260,0.000888,0.00293,n2o5_fit[0][0]]),array([0.0000089,0.0000073,0.000023,0.000067,0.00021,sqrt(n2o5_fit[1][0,0])])])\n",
      "ktemp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errorbar(ktemp[0],ktemp[1],yerr=ktemp[2],marker='o',linestyle='None')\n",
      "xlabel('T/K')\n",
      "ylabel('$k/s^{-1}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arrhenius(T,Ea,A):\n",
      "    \"Exponential function taking parameters of Arrhenius form in SI units.\"\n",
      "    R=8.3144621\n",
      "    return A*e**(-Ea/(R*T))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arr_fit=scipy.optimize.curve_fit(arrhenius,ktemp[0],ktemp[1],sigma=ktemp[2],p0=[8e4,1e10])\n",
      "arr_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqrt(diag(arr_fit[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T=linspace(290,340,50)\n",
      "\n",
      "errorbar(ktemp[0],ktemp[1],yerr=ktemp[2],marker='o',linestyle='None')\n",
      "plot(T,arrhenius(T,arr_fit[0][0],arr_fit[0][1]),linestyle='--')\n",
      "xlabel('T/K')\n",
      "ylabel('$k/s^{-1}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you are finding difficulties finding a suitable initial guess for the non-linear Arrhenius fit, or if on the other hand you got your answers quickly, let's try a different approach.\n",
      "\n",
      "The Arrhenius equation is non-linear function of $T$, but the equation can be converted into an amenable for linear fit. This can be done by taking the natural logarithm of the Arrhenius equation:\n",
      "\n",
      "$$\\ln(k)=\\ln(A)-\\frac{E_a}{R}\\frac{1}{T} .$$\n",
      "\n",
      "We have just obtained a linear relationship between $\\ln(k)$ and $\\frac{1}{T}$, where the intercept is $\\ln(A)$ and the slope $-\\frac{E_a}{R}$. It should be noted that only in few cases is it possible to transform a non-linear fit into a linear one in this way.\n",
      "\n",
      "* From the data, plot the of $\\ln(k)$ as a function of $\\frac{1}{T}$, and fit it to a linear function (do not consider error bars in this fit). How do your results compare with the non-linear fit above?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_nerr=polyfit((1/ktemp[0]),log(ktemp[1]),1,cov=True)\n",
      "klinfit_nerr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_nerr_line=klinfit_nerr[0][1]+klinfit_nerr[0][0]*(1/ktemp[0])\n",
      "\n",
      "plot((1/ktemp[0]),log(ktemp[1]),marker='o',linestyle='None')\n",
      "plot((1/ktemp[0]),klinfit_nerr_line,linstyle='--')\n",
      "xlable('1/T /K')\n",
      "ylabel(r'$\\ln(k) /\\ln(s^{-1})$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous question we performed a fit without taking the error bars of the data into account. This is equivalent to considering all the $\\ln(k)$ data point have the same uncertainty. From the $T$ vs $k$ table above we know this is not the case, and we should take the uncertainties into account. Note however that $\\sigma_k$ is not the same as $\\sigma_{\\ln(k)}$, and also $\\sigma_{\\ln(k)}\\neq\\ln(\\sigma_k)$! There are [general methods](http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Simplification) to propagate the uncertainty of a value upon a mathematical operation. For the present purposes however, we will focus on the case of the logarithm at hand:\n",
      "\n",
      "$$\\sigma_{\\ln(k)}=\\frac{\\sigma_k}{k} .$$\n",
      "\n",
      "* Plot the values of $\\ln(k)$ as a function of $\\frac{1}{T}$ with appropriate error bars. Note the values of the error bars at low $T$ values. What do you expect the relative contribution of these data points to the linear fit to be?\n",
      "\n",
      "* Fit the data taking into account the error bars (recall the differences between functions *polyfit* and *curve_fit* in dealing with data points uncertainty). How do the results obtained via this proceedure compare with the ones obtained via direct exponential fit?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_err=polyfit((1/ktemp[0]),log(ktemp[1]),1,w=1/(ktemp[2]/ktemp[0])**2,cov=True)\n",
      "klinfit_err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_err_line=klinfit_err[0][1]+klinfit_err[0][0]*(1/ktemp[0])\n",
      "\n",
      "errbar((1/ktemp[0]),log(ktemp[1]),yerr=(ktemp[2]/ktemp[0]),marker='o',linestyle='None')\n",
      "plot((1/ktemp[0]),klinfit_err_line,linstyle='--')\n",
      "xlable('1/T /K')\n",
      "ylabel(r'$ln(k) /ln(s^{-1})$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}