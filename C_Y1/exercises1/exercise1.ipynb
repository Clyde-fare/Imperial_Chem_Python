{
 "metadata": {
  "name": "",
  "signature": "sha256:eb28e36f13526f09e70e3428c111a6dcce5ab5611dbf2b1a38ac20a8b5ee9329"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problem set 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Clyde Fare](mailto:c.fare12@imperial.ac.uk) and [Jo\u00e3o Pedro Malhado](mailto:malhado@imperial.ac.uk), Imperial College London\n",
      "\n",
      "Notebook is adapted from material gathered by Andrew McKinley and Oliver Robotham, as well as Paul Wilde, and is licensed under a [Creative Commons Attribution 3.0 (CC-by) license](http://creativecommons.org/licenses/by/3.0/us)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load the pylab and non-linear fitting machinery and get that out of the way."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we'll be dealing with larger datasets, such as those you could have obtained from an automatic measuring instrument, and looking at some statistical properties of sets of measurements.\n",
      "\n",
      "The file [measurements.csv](files/measurements.csv) contains 5 columns of data corresponding to 100,000 repeated measurements of the same system in an identical state using 5 different measuring instruments (good thing we don't have to analyse this data by hand!)\n",
      "\n",
      "* Have a peek at the file and load its data into a variable. Output the value of the variable to see how the imported data looks like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = loadtxt('measurements.csv',delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A quick and informative way of acquiring an overall view of the data distribution is to plot histograms.\n",
      "\n",
      "* On the same figure, plot the histograms of all 5 sets of data labelling each of them (make sure the figure is of reasonable size and a legend is present). \\*\n",
      "\n",
      "\\* A better visualisation is obtained by setting the range for each histogram to be equal, and setting the transparency alpha value to less than 1 so that we see through superimposed histograms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figsize(20,15)\n",
      "\n",
      "hist(data[:,0], 200, range=(-4,10), alpha=0.5, label='data1')\n",
      "hist(data[:,1], 200, range=(-4,10), alpha=0.5, label='data2')\n",
      "hist(data[:,2], 200, range=(-4,10), alpha=0.5, label='data3')\n",
      "hist(data[:,3], 200, range=(-4,10), alpha=0.5, label='data4')\n",
      "hist(data[:,4], 200, range=(-4,10), alpha=0.5, label='data5')\n",
      "\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measurements are subject to noise thus two consecutive measurements of the same system are unlikely to give exactly the same result even if the system is unchanged, we see this in our dataset.\n",
      "\n",
      "The *mean* $\\bar{x}$ of a set of independent measurements of some property $x$ is the best estimate of the actual value of that property. As we will see below, the precision of this estimate can be improved by increasing the number of measurements we are averaging over. \n",
      "\n",
      "The *standard deviation* $s_x$ quantifies the spread of the experimental measurements about its mean value. (When writing up experiments you will often report the mean of some set of measurements +/- some error value, it is important to note that the standard deviation is *not* this error value - below we will examine what this error is in more detail.)\n",
      "\n",
      "* For each instrument's 100,000 measurements, calculate the mean and the standard deviation (don't forget to set the appropriate *ddof* value). \n",
      "\n",
      "* How do the $\\bar{x}$ and $s_x$ values relate the shape of the distributions in the histograms?\n",
      "\n",
      "* Construct another figure containing the histograms as above but with the following addition: using the mean values you have calculated, add vertical lines marking out the position of the mean for the 5 distributions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(data, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "std(data, ddof=1, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figsize(20,15)\n",
      "\n",
      "hist(data[:,0], 200, range=(-4,10), alpha=0.5, label='data1')\n",
      "plot(array([mean(data[:,0]), mean(data[:,0])]), array([0,30000]), linestyle='--', color='black')\n",
      "\n",
      "##alternative way to plot vertical line\n",
      "#axvline(x=mean(data[:,0]), ymin=0, ymax=3000, linestyle='--', color='black')\n",
      "    \n",
      "hist(data[:,1], 200, range=(-4,10), alpha=0.5, label='data2')\n",
      "plot(array([mean(data[:,1]), mean(data[:,1])]), array([0,30000]), linestyle='--', color='black')\n",
      " \n",
      "hist(data[:,2], 200, range=(-4,10), alpha=0.5, label='data3')\n",
      "plot(array([mean(data[:,2]), mean(data[:,2])]), array([0,30000]), linestyle='--', color='black')\n",
      "\n",
      "hist(data[:,3], 200, range=(-4,10), alpha=0.5, label='data4')\n",
      "plot(array([mean(data[:,3]), mean(data[:,3])]), array([0,30000]), linestyle='--', color='black')\n",
      "\n",
      "hist(data[:,4], 200, range=(-4,10), alpha=0.5, label='data5')\n",
      "plot(array([mean(data[:,4]), mean(data[:,4])]) ,array([0,30000]), linestyle='--', color='black')\n",
      "\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now study how some statistical properties vary with sample size.\n",
      "\n",
      "* For the data from the first measuring instrument, construct three histograms on a single figure - one with all 100,000 measurements, one for the first 10,000 measurements, and another for the first 1,000 measurements.\n",
      "\n",
      "* Change figure so that are normalized by sample size (you will need to use the built in tools to discover the appropriate keyword of function **hist** to achieve this).\n",
      "\n",
      "By looking at these three histograms, and by calculating their means and standard deviations, what do you notice about how much the mean value, and the standard deviation change as the sample size grows? (bear in mind that all these sample sizes are all large)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist(data[:,0], 100, normed=True, range=(0,5), alpha=0.5,label='N=100000')\n",
      "hist(data[:10000,0], 100, normed=True, range=(0,5), alpha=0.5,label='N=10000')\n",
      "hist(data[:1000,0], 100, normed=True, range=(0,5), alpha=0.5,label='N=1000')\n",
      "\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "array([mean(data[:1000,0]), mean(data[:10000,0]), mean(data[:,0])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "array([std(data[:1000,0],ddof=1), std(data[:10000,0],ddof=1), std(data[:,0],ddof=1)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statistical uncertainty associated with the mean value is given by the *standard error of the mean* $\\sigma_{\\bar{x}}$ which is calculated by the expression:\n",
      "\n",
      "$$\\sigma_{\\bar{x}}=\\frac{s_x}{\\sqrt{N}}=\\sqrt{\\frac{1}{N (N-1)}\\sum_i^N (x_i-\\bar{x})^2} .$$\n",
      "\n",
      "This is the value that should be reported as the statistical error of your measurements.\n",
      "\n",
      "Let us investigate how this quantity varies with sample size. We will be considering data from instruments 1 and 4, these have similar mean value as you should see in your histograms above.\n",
      "\n",
      "* For datasets 1 and 4 pick 5 sample sizes between 1000 and 100,000 and produce a plot showing how the standard deviation and the standard error change as sample size is varied.\\*\n",
      "\n",
      "* What does this plot tell you about the usefulness of repeating the same measurement?\n",
      "\n",
      "\\* You may want to use a logarithmic scale in this plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = array([1000,25000,50000,75000,100000])\n",
      "\n",
      "std_sample_1 = array([std(data[:1000,0],ddof=1),\n",
      "                      std(data[:25000,0],ddof=1),\n",
      "                      std(data[:50000,0],ddof=1),\n",
      "                      std(data[:75000,0],ddof=1),\n",
      "                      std(data[:,0],ddof=1)])\n",
      "\n",
      "ste_sample_1 = array([std(data[:1000,0],ddof=1)/sqrt(1000),\n",
      "                      std(data[:25000,0],ddof=1)/sqrt(25000),\n",
      "                      std(data[:50000,0],ddof=1)/sqrt(50000),\n",
      "                      std(data[:75000,0],ddof=1)/sqrt(75000),\n",
      "                      std(data[:,0],ddof=1)/sqrt(100000)])\n",
      "\n",
      "std_sample_4 = array([std(data[:1000,3],ddof=1),\n",
      "                      std(data[:25000,3],ddof=1),\n",
      "                      std(data[:50000,3],ddof=1),\n",
      "                      std(data[:75000,3],ddof=1),\n",
      "                      std(data[:,3],ddof=1)])\n",
      "\n",
      "ste_sample_4 = array([std(data[:1000,3],ddof=1)/sqrt(1000),\n",
      "                      std(data[:25000,3],ddof=1)/sqrt(25000),\n",
      "                      std(data[:50000,3],ddof=1)/sqrt(50000),\n",
      "                      std(data[:75000,3],ddof=1)/sqrt(75000),\n",
      "                      std(data[:,3],ddof=1)/sqrt(100000)])\n",
      "\n",
      "plot(N, std_sample_1, color='blue', linestyle='--', label=r'$s_x(data1)$')\n",
      "plot(N, ste_sample_1, color='blue', label=r'$\\sigma_{\\bar{x}}(data1)$')\n",
      "plot(N, std_sample_4, color='cyan', linestyle='--', label=r'$s_x(data4)$')\n",
      "plot(N, ste_sample_4, color='cyan', label=r'$\\sigma_{\\bar{x}}(data4)$')\n",
      "\n",
      "xlabel('Sample Size')\n",
      "yscale('log')\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the histograms above we note that the spread of data set 4 is greater than that of data set 1 (i.e. instrument 4 is more affected by noise than instrument 1), and consequently at equal sample sizes the standard error of the mean is greater for dataset 4.\n",
      "\n",
      "* By drawing a horizontal line in the plot above do a rough estimate of what sample size is needed to achieve the same precision using measuring instrument 1, as is obtained using measuring instrument 4 with 100,000 measurements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = array([1000,25000,50000,75000,100000])\n",
      "\n",
      "plot(N, std_sample_1, color='blue', linestyle='--',label=r'$s_x(data1)$')\n",
      "plot(N, ste_sample_1, color='blue', label=r'$\\sigma_{\\bar{x}}(data1)$')\n",
      "plot(N, std_sample_4, color='cyan', linestyle='--',label=r'$s_x(data4)$')\n",
      "plot(N, ste_sample_4, color='cyan', label=r'$\\sigma_{\\bar{x}}(data4)$')\n",
      "\n",
      "plot(array([0,100000]),\n",
      "     array([std(data[:,3],ddof=1)/sqrt(100000),\n",
      "            std(data[:,3],ddof=1)/sqrt(100000)]),\n",
      "     color='red',linestyle=':')\n",
      "\n",
      "yscale('log')\n",
      "legend()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#takes a while to generate the statistics should probably do it once and write it out to a file\n",
      "ms,stds,stes = [],[],[]\n",
      "for d in [0,3]:\n",
      "    instrument_ms   = array( [mean(data[:i,d]       )     for i in range(2, len(data))] )\n",
      "    instrument_stds = array( [ std(data[:i,d],ddof=1)     for i in range(2, len(data))] )\n",
      "    instrument_stes = array( [instrument_stds[i-2]/i**0.5 for i in range(2, len(data))] )\n",
      "    \n",
      "    ms.append(instrument_ms)\n",
      "    stds.append(instrument_stds)\n",
      "    stes.append(instrument_stes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desired_e = stes[1][-1]\n",
      "sample_sizes = flatnonzero(stes[0] < desired_e)\n",
      "no_samples_needed = sample_sizes[0]\n",
      "no_samples_needed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.html.widgets import interact\n",
      "\n",
      "@interact(plt_range=[0,100000,1000])\n",
      "def plot_function(plt_range=8000):\n",
      "\n",
      "    for i,d in enumerate([0,3]):\n",
      "        plot(  ms[i], label='Instrument {} mean'.format(d+1))\n",
      "        plot(stds[i], label='Instrument {} standard deviation'.format(d+1))\n",
      "        plot(stes[i], label='Instrument {} standard error'.format(d+1))\n",
      "        \n",
      "        ##statistics start at a sample size of two\n",
      "        #plot(N, stes[i][N-3], linestyle='', marker='o')\n",
      "    \n",
      "    axhline(y=desired_e, linestyle='--', color='black')\n",
      "    plot([no_samples_needed]*2, [ylim()[0], desired_e],linestyle='--')\n",
      "    \n",
      "    xlabel('Sample Size')\n",
      "    xlim([0,plt_range])\n",
      "    yscale('log')\n",
      "    legend()\n",
      "    show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a final note, from the definition of standard error and the study we just carried out, what is the limit of the standard error value as the sample size goes to infinity? \n",
      "\n",
      "Although this statistical error vanishes for infinite sample sizes, this does not mean you can achieve absolute confidence in your measurements by taking an extremely large number of repetitions. \n",
      "\n",
      "The standard error quantifies our uncertainty due to the presence of random noise, as we have seen we can combat this using a sufficiently large sample size making our estimate more and more precise. There is however a second form of error that influences the accuracy of our estimate and this form is not reduced through repetition: *systematic error*. This corresponds to imperfect/faulty operation of the instruments or miscalibration.\n",
      "\n",
      "Looking at the data produced by our five instruments we see that all the instruments are affected by noise with instrument 5 the most precise but that instruments two three and five suffer from systematic errors which push their mean value of their measurements away from the actual value of the system (~2.4) which is correctly captured by instruments one and four. \n",
      "\n",
      "Systematic errors need to be estimated and corrected for based on knowledge of the measurement process and instruments used."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we will be studying the kinetics of the decomposition reaction of $N_2 O_5$\n",
      "\n",
      "$$2 N_2 O_{5 (g)} \\longrightarrow 4NO_{2 (g)} + O_{2 (g)}$$\n",
      "\n",
      "The rate of a chemical reaction is how fast the concentration of one of the reactants or products changes during the chemical reaction. In the case of the reaction under study the rate of the reaction is proportional to the concentration of $N_2 O_5$, it is said to obey a first order rate law:\n",
      "\n",
      "$$rate=- \\frac{d[N_2 O_5]}{d t} =k [N_2 O_5] ,$$\n",
      "\n",
      "where $k$ is the *rate constant*. This is a differential equation and its solution gives us the concentration of $N_2 O_5$ as a function of time.\n",
      "\n",
      "$$[N_2 O_5]=[N_2 O_5]_0 e^{-k t}\ufffc$$\n",
      "\n",
      "where $[N_2 O_5]_0$ is the initial concentration and $t$ is the time. (If you want to know where the solution comes from click [here](https://wiki.ch.ic.ac.uk/wiki/index.php?title=Solving_the_Rate_Equation).)\n",
      "\n",
      "We will attemp to determine the value of the rate constant $k$. In order to do this, the concentration of $N_2 O_5$ was recorded as a function of time at $65^{\\circ}C$, and recorded in file [N2O5vst_65C.dat](files/N2O5vst_65C.dat), where the first column is time in seconds and the second column is concentration in $\\text{mol.dm}^{-3}$.\n",
      "\n",
      "* Load the data and plot the concentration of $N_2 O_5$ against time, label the axes and include the units in the label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t,c = loadtxt('N2O5vst_65C.dat').T\n",
      "\n",
      "plot(t,c,marker='o', linestyle='')\n",
      "xlabel('t/s')\n",
      "ylabel('$[N_2 O_5]/mol.dm^{-3}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Create an appropriate python function for the variation of $[N_2 O_5]$ with time that through non linear fitting will allow us to extract the rate constant\n",
      "\n",
      "* Fit the function to the data and report the value of the rate constant and the uncertainty associated with it. \n",
      "\n",
      "* Superimpose the fit with the experimental data to check your fit is meaningful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rate1(t,k,C0):\n",
      "    \"Exponential function consistent with first order rate law.\"\n",
      "    return C0*e**(-k*t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n2o5_fit = scipy.optimize.curve_fit(rate1, t, c, p0=[0.000001, 0.1])\n",
      "n2o5_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqrt(n2o5_fit[1][0,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k=n2o5_fit[0][0]\n",
      "C0=n2o5_fit[0][1]\n",
      "\n",
      "plot(t,c,marker='o',linestyle='none')\n",
      "plot(t,rate1(t,k,C0),linestyle='--')\n",
      "xlabel('t/s')\n",
      "ylabel('$[N_2 O_5]/mol.dm^{-3}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some other colleagues followed the same proceedure as above but at different temperatures. The following table shows values of the rate constant and its uncertainty for different temperatures.\n",
      "\n",
      "<table>\n",
      "<tr><th>$T/^{\\circ} C$</th><th>$k/s^{-1}$</th><th>$\\sigma_k/s^{-1}$</th><tr>\n",
      "<tr><td>20</td><td>0.0000188</td><td>0.0000089</td></tr>\n",
      "<tr><td>30</td><td>0.0000694</td><td>0.0000073</td></tr>\n",
      "<tr><td>40</td><td>0.000260</td><td>0.000023</td></tr>\n",
      "<tr><td>50</td><td>0.000888</td><td>0.000067</td></tr>\n",
      "<tr><td>60</td><td>0.00293</td><td>0.00021</td></tr>\n",
      "</table>\n",
      "\n",
      "The variation of the rate constant with temperature can give important information about the chemical reaction. The rate constant of an elementary reaction varies in general with temperature according to the Arrhenius equation that you will cover in more detail in your kinetics lectures\n",
      "\n",
      "$$k(T)=A e^{-\\frac{E_a}{R T}} ,$$\n",
      "\n",
      "Where $E_a$ is activation energy, $A$ is the so called pre-exponential factor and $R$ is the ideal gas constant.\n",
      "\n",
      "* Create an appropriate function that through non linear fitting will allow us to extract the pre-exponential factor and the rate constant.\n",
      "\n",
      "* From the rate constant values on the table above, and the rate constant value you have determined, fit the data to the determine the pre-exponential factor and the activation energy for this reaction.\\* \n",
      "\n",
      "\\* Notice that the uncertainties associated with each rate constant are different, you should take this into account."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ktemp=array([array([20,30,40,50,60,65])+273.15,\n",
      "             array([0.0000188,0.0000694,0.000260,0.000888,0.00293,n2o5_fit[0][0]]),\n",
      "             array([0.0000089,0.0000073,0.000023,0.000067,0.00021,sqrt(n2o5_fit[1][0,0])])])\n",
      "ktemp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errorbar(ktemp[0],ktemp[1],yerr=ktemp[2],marker='o',linestyle='None')\n",
      "xlabel('T/K')\n",
      "ylabel('$k/s^{-1}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arrhenius(T,Ea,A):\n",
      "    \"Exponential function taking parameters of Arrhenius form in SI units.\"\n",
      "    R=8.3144621\n",
      "    return A*e**(-Ea/(R*T))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arr_fit=scipy.optimize.curve_fit(arrhenius,ktemp[0],ktemp[1],sigma=ktemp[2],p0=[8e4,1e10])\n",
      "arr_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqrt(diag(arr_fit[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T=linspace(290,340,50)\n",
      "\n",
      "errorbar(ktemp[0],ktemp[1],yerr=ktemp[2],marker='o',linestyle='None')\n",
      "plot(T,arrhenius(T,arr_fit[0][0],arr_fit[0][1]),linestyle='--')\n",
      "xlabel('T/K')\n",
      "ylabel('$k/s^{-1}$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For those having difficulties finding a suitable initial guess for the non-linear Arrhenius fit, or if you managed to extract your activation energy quickly try a different approach:\n",
      "\n",
      "The Arrhenius equation is a non-linear function of $T$, but can be converted into a linear form by taking the natural logarithm of both sides of the equation:\n",
      "\n",
      "$$\\ln(k)=\\ln(A)-\\frac{E_a}{R}\\frac{1}{T} .$$\n",
      "\n",
      "We obtain a linear relationship between $\\ln(k)$ and $\\frac{1}{T}$, where the intercept is $\\ln(A)$ and the slope $-\\frac{E_a}{R}$. It should be noted that it is only possible in a few cases to transform a non-linear equation into a linear one in this way.\n",
      "\n",
      "* Plot $\\ln(k)$ against $\\frac{1}{T}$\n",
      "* Use polyfit to extract the gradient and intercept of the $\\ln(k)$ vs. $\\frac{1}{T}$ data (ignore any uncertainties associated with the data when performing this fit) \n",
      "* Use these to determine the activation energy and the pre-exponential factor. \n",
      "* How do your results compare with the non-linear fit above?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_nerr=polyfit((1/ktemp[0]),log(ktemp[1]),1,cov=True)\n",
      "klinfit_nerr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_nerr_line=klinfit_nerr[0][1]+klinfit_nerr[0][0]*(1/ktemp[0])\n",
      "\n",
      "plot((1/ktemp[0]),log(ktemp[1]),marker='o',linestyle='None')\n",
      "plot((1/ktemp[0]),klinfit_nerr_line,linestyle='--')\n",
      "xlabel('1/T /K')\n",
      "ylabel(r'$\\ln(k) /\\ln(s^{-1})$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous question we performed a fit without taking the uncertainties assoicated with the data into account. This is equivalent to considering all the $\\ln(k)$ data points have the same uncertainty. From the $T$ vs $k$ table above we know this is not the case, and we should take the uncertainties into account. Note however that $\\sigma_k$ is not the same as $\\sigma_{\\ln(k)}$, and also $\\sigma_{\\ln(k)}\\neq\\ln(\\sigma_k)$! There are [general methods](http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Simplification) to propagate the uncertainty of a value upon a mathematical operation. For the present purposes however, we will focus on the case of the logarithm at hand:\n",
      "\n",
      "$$\\sigma_{\\ln(k)}=\\frac{\\sigma_k}{k} .$$\n",
      "\n",
      "* Plot the values of $\\ln(k)$ as a function of $\\frac{1}{T}$ with appropriate error bars. Note the values of the error bars at low $T$ values. What do you expect the relative contribution of these data points to the linear fit to be?\n",
      "\n",
      "* Fit the data taking into account the error bars (recall the differences between functions *polyfit* and *curve_fit* in dealing with data point uncertainty). How do the results obtained via this proceedure compare with the ones obtained via direct exponential fit?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_err=polyfit((1/ktemp[0]),log(ktemp[1]),1,w=1/(ktemp[2]/ktemp[1])**2,cov=True)\n",
      "klinfit_err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "klinfit_err_line=klinfit_err[0][1]+klinfit_err[0][0]*(1/ktemp[0])\n",
      "\n",
      "errorbar((1/ktemp[0]),log(ktemp[1]),yerr=(ktemp[2]/ktemp[1]),marker='o',linestyle='None')\n",
      "plot((1/ktemp[0]),klinfit_err_line,linestyle='--')\n",
      "xlabel('1/T /K')\n",
      "ylabel(r'$ln(k) /ln(s^{-1})$')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}