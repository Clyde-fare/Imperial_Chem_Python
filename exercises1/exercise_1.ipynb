{
 "metadata": {
  "name": "",
  "signature": "sha256:5405f5704863a74b5b44b2ac5fb5b110c353a6d259b9247553b068f9f29d0b1b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problem set 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Clyde Fare](mailto:python@imperial.ac.uk) and [Jo\u00e3o Pedro Malhado](mailto:python@imperial.ac.uk), Imperial College London\n",
      "\n",
      "This notebook is adapted from material by Andrew McKinley, Niall Jackson, Oliver Robotham, and Paul Wilde, and is licensed under a [Creative Commons Attribution 3.0 (CC-by) license](http://creativecommons.org/licenses/by/3.0/us)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load the pylab and non-linear fitting machinery and get that out of the way."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we'll be dealing with larger datasets, such as those you could have obtained from an automatic measuring instrument, and looking at some statistical properties of sets of measurements.\n",
      "\n",
      "The file [measurements.csv](files/measurements.csv) contains the data we will be analysing in these exercises. (**Note** a .csv file is a text file and you can treat it in an identical fashion to a .txt file.) \n",
      "\n",
      "The file contains 5 columns of data corresponding to 100,000 repeated measurements of the same system using 5 different measuring instruments (good thing we don't have to analyse this data by hand!)\n",
      "\n",
      "* Have a look at the file (using a text editor e.g. notepad) and load its data into a variable. Output the value of the variable to see how the imported data looks like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A quick and informative way of acquiring an overall view of the data distribution is to plot histograms.\n",
      "\n",
      "* On the same figure, plot 5 histograms of 100 bins for each of the 5 sets of data.\n",
      "* Label the figure and make sure the it is of reasonable size and a legend is present.\n",
      "* Set the *range* keyword of each histogram to be equal such that all histograms are visible, and set the transparency *alpha* keyword to a value less than 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measurements are subject to noise thus two consecutive measurements of the same system are unlikely to give exactly the same result even if the system is unchanged, we see this in our dataset.\n",
      "\n",
      "The *mean* $\\bar{x}$ of a set of independent measurements of some property $x$ is the best estimate of the actual value of that property. As we will see below, the precision of this estimate can be improved by increasing the number of measurements we are averaging over. \n",
      "\n",
      "The *standard deviation* $s_x$ quantifies the spread of the experimental measurements about its mean value. (It corresponds to the average error of a single measurement. When writing up experiments you will often report the mean of some set of measurements +/- some error value, it is important to note that the standard deviation is *not* this error value - below we will examine what this error is in more detail.)\n",
      "\n",
      "* For each of the 5 datasets, calculate the mean and the standard deviation (don't forget to set the appropriate *ddof* value). \n",
      "\n",
      "* How do the $\\bar{x}$ and $s_x$ values relate the shape of the distributions in the histograms?\n",
      "\n",
      "* Construct another figure containing the histograms as above but with the following addition: using the mean values you have calculated, add vertical lines marking out the position of the mean for the 5 distributions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now study how some statistical properties vary with sample size.\n",
      "\n",
      "* For the 1st dataset, construct three histograms on a single figure - one with all 100,000 measurements, one for the first 10,000 measurements, and another for the first 1,000 measurements.\n",
      "\n",
      "* Change the figure so that the histograms are normalized (you will need to use the built in tools to discover the appropriate keyword of function **hist** to achieve this).\n",
      "\n",
      "By looking at these three histograms, and by calculating their means and standard deviations, what do you notice about how much the mean value, and the standard deviation change as the sample size grows? (Bear in mind that all these sample sizes are fairly large.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statistical uncertainty associated with the mean value is given by the *standard error of the mean* $\\sigma_{\\bar{x}}$ which is calculated by the expression:\n",
      "\n",
      "$$\\sigma_{\\bar{x}}=\\frac{s_x}{\\sqrt{N}}=\\sqrt{\\frac{1}{N (N-1)}\\sum_i^N (x_i-\\bar{x})^2} .$$\n",
      "\n",
      "This is the value that should be reported as the statistical error of your measurements.\n",
      "\n",
      "Let us investigate how this quantity varies with sample size. We will be considering data from instruments 1 and 4, these have similar mean value as you should see in your histograms above.\n",
      "\n",
      "* For datasets 1 and 4, choose 5 sample sizes between 1000 and 100,000 calculate the standard deviation and the standard error.\n",
      "\n",
      "* For the two datasets above, plot the standard deviation and standard error against the sample size. (You may want to use a logarithmic *y* scale in this plot.)\n",
      "\n",
      "* What does this plot tell you about the usefulness of repeating the same measurement?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* By drawing a horizontal line in the plot above, do a rough estimate of what sample size is needed to achieve the same precision using measuring instrument 1, as is obtained using measuring instrument 4 with 100,000 measurements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a final note, from the definition of standard error and the study we just carried out, what is the limit of the standard error value as the sample size goes to infinity? \n",
      "\n",
      "Although this statistical error vanishes for infinite sample sizes, this does not mean you can achieve absolute confidence in your measurements by taking an extremely large number of repetitions. \n",
      "\n",
      "The standard error quantifies our uncertainty due to the presence of random noise. As we have seen we can combat this using a sufficiently large sample size making our estimate more and more precise. There is however a second form of error that influences the accuracy of our estimate and this form is not reduced through repetition: *systematic error*. This corresponds to imperfect/faulty operation of the instruments or miscalibration.\n",
      "\n",
      "Looking at the data produced by our five instruments we see that all the instruments are affected by noise (instrument 5 being the most precise) but that instruments two three and five suffer from systematic errors which push the mean value of their measurements away from the actual value of the system (~2.4) which is correctly captured by instruments one and four. \n",
      "\n",
      "Systematic errors need to be estimated and corrected for based on knowledge of the measurement process and instruments used."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we will be studying the kinetics of the decomposition reaction of $N_2 O_5$\n",
      "\n",
      "$$2 N_2 O_{5 (g)} \\longrightarrow 4NO_{2 (g)} + O_{2 (g)}$$\n",
      "\n",
      "The rate of a chemical reaction is how fast the concentration of one of the reactants or products changes during the chemical reaction. In the case of the reaction under study the rate of the reaction is proportional to the concentration of $N_2 O_5$, it is said to obey a first order rate law:\n",
      "\n",
      "$$rate=- \\frac{d[N_2 O_5]}{d t} =k [N_2 O_5] ,$$\n",
      "\n",
      "where $k$ is the *rate constant*. This is a differential equation and its solution gives us the concentration of $N_2 O_5$ as a function of time.\n",
      "\n",
      "$$[N_2 O_5]=[N_2 O_5]_0 e^{-k t}\ufffc$$\n",
      "\n",
      "where $[N_2 O_5]_0$ is the initial concentration and $t$ is the time. (If you want to know where the solution comes from click [here](https://wiki.ch.ic.ac.uk/wiki/index.php?title=Solving_the_Rate_Equation).)\n",
      "\n",
      "We are interested in determining the value of the rate constant $k$. In order to do this, the concentration of $N_2 O_5$ was recorded as a function of time at $65^{\\circ}C$, and stored in file [N2O5vst_65C.dat](files/N2O5vst_65C.dat), where the first column is time in seconds and the second column is concentration in $\\text{mol.dm}^{-3}$.\n",
      "\n",
      "* Load the data and plot the concentration of $N_2 O_5$ against time, label the axes and include the units in the label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Create an appropriate python function for the variation of $[N_2 O_5]$ with time that through non linear fitting will allow us to extract the rate constant\n",
      "\n",
      "* Fit the function to the data and report the value of the rate constant and the uncertainty associated with it. \n",
      "\n",
      "* Superimpose the fit with the experimental data to check your fit is meaningful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some other colleagues followed the same proceedure as above but at different temperatures. The following table shows values of the rate constant and its uncertainty for different temperatures.\n",
      "\n",
      "<table>\n",
      "<tr><th>$T/^{\\circ} C$</th><th>$k/s^{-1}$</th><th>$\\sigma_k/s^{-1}$</th><tr>\n",
      "<tr><td>20</td><td>0.0000188</td><td>0.0000089</td></tr>\n",
      "<tr><td>30</td><td>0.0000694</td><td>0.0000073</td></tr>\n",
      "<tr><td>40</td><td>0.000260</td><td>0.000023</td></tr>\n",
      "<tr><td>50</td><td>0.000888</td><td>0.000067</td></tr>\n",
      "<tr><td>60</td><td>0.00293</td><td>0.00021</td></tr>\n",
      "</table>\n",
      "\n",
      "The variation of the rate constant with temperature can give important information about the chemical reaction. The rate constant of an elementary reaction varies in general with temperature according to the Arrhenius equation that you will cover in more detail in your kinetics lectures\n",
      "\n",
      "$$k(T)=A e^{-\\frac{E_a}{R T}} ,$$\n",
      "\n",
      "where $E_a$ is activation energy, $A$ is the so called pre-exponential factor (which is related to the frequency of molecular collisions) and $R$ is the ideal gas constant.\n",
      "\n",
      "* Create an appropriate function that through non linear fitting will allow us to extract the pre-exponential factor and the activation energy.\n",
      "\n",
      "* From the rate constant values on the table above, and the rate constant value you have determined, fit the data to the determine the pre-exponential factor and the activation energy for this reaction. \n",
      "\n",
      "* Plot values predicted from your fitted function alongside the experimental values to visualise your result.\n",
      "\n",
      "*Note 1*: The uncertainties associated with each rate constant are different, you should take this into account. \n",
      "\n",
      "*Note 2*: Units of temperature must be consistent with units of R."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you are having difficulties finding a suitable initial guess for the non-linear Arrhenius fit, or if you managed to extract your activation energy quickly try a different approach:\n",
      "\n",
      "The Arrhenius equation is a non-linear function of $T$, but can be converted into a linear form by taking the natural logarithm of both sides of the equation:\n",
      "\n",
      "$$\\ln(k)=\\ln(A)-\\frac{E_a}{R}\\frac{1}{T} .$$\n",
      "\n",
      "We obtain a linear relationship between $\\ln(k)$ and $\\frac{1}{T}$, where the intercept is $\\ln(A)$ and the slope $-\\frac{E_a}{R}$. It should be noted that it is only possible in a few cases to transform a non-linear equation into a linear one in this way.\n",
      "\n",
      "* Plot $\\ln(k)$ against $\\frac{1}{T}$\n",
      "* Use polyfit to extract the gradient and intercept of the $\\ln(k)$ vs. $\\frac{1}{T}$ data (ignore any uncertainties associated with the data when performing this fit) \n",
      "* Use these to determine the activation energy and the pre-exponential factor. \n",
      "* How do your results compare with the non-linear fit above?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous question we performed a fit without taking the uncertainties assoicated with the data into account. This is equivalent to considering all the $\\ln(k)$ data points have the same uncertainty. From the $T$ vs $k$ table above we know this is not the case, and we should take the uncertainties into account. Note however that $\\sigma_k$ is not the same as $\\sigma_{\\ln(k)}$, and also $\\sigma_{\\ln(k)}\\neq\\ln(\\sigma_k)$! There are [general methods](http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Simplification) to propagate the uncertainty of a value upon a mathematical operation. For the present purposes however, we will focus on the case of the logarithm at hand:\n",
      "\n",
      "$$\\sigma_{\\ln(k)}=\\frac{\\sigma_k}{k} .$$\n",
      "\n",
      "* Plot the values of $\\ln(k)$ as a function of $\\frac{1}{T}$ with appropriate error bars. Note the values of the error bars at low $T$ values. What do you expect the relative contribution of these data points to the linear fit to be?\n",
      "\n",
      "* Fit the data taking into account the error bars (recall the differences between functions *polyfit* and *curve_fit* in dealing with data point uncertainty). How do the results obtained via this proceedure compare with the ones obtained via direct exponential fit?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}